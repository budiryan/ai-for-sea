# AI For SEA: Ride Safety
My solution to Grab's AI for SEA challenge: "Ride Safety" track

## Requirements
1. Python 3 (version >= 3.6 recommended)
2. `pip`
3. `virtualenv`

## Setup
1. Install Python 3 on your system
2. Install `pip`
3. Install `virtualenv`: `pip install virtualenv`
4. Generate virtual environment directory: `virtualenv env`. Please note that your `virtualenv` SHOULD generate a `python 3` environment.
   If `python 2` is generated instead, execute `virtualenv -p python3 env` instead.
5. Activate virtual environment:
    * Mac / Linux: `source env/bin/activate`
    * Windows: `env\Scripts\activate.bat`
6. Install dependencies: `pip install -r requirements.txt`
7. Note: This project uses LightGBM as one of the models. For Mac users, starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.
Therefore, before installing dependencies, you must `brew install libomp` first.

## Predicting Your Hold-Out Test Set
1. The models have already been pre-trained so you do not need to place any train data, only test feature data is needed
2. Place the folder containing the test CSV files inside the root project directory. 
   Please make sure the folder is STERILE and contains the test CSV files only.
3. `python predict_test.py --source/-s [PATH_TO_TEST_FOLDER] --destination/-d [PATH_TO_PREDICTION_CSV_FILE, e.g.: predictions.csv]`
    
    Example: `python predict_test.py -s test_folder/ -d predictions.csv`
4. Once successful, prediction file will be generated at the path you specified.

## Project Presentation
### Feature Engineering
For the code, head to [features](features/) folder for full information on the implementation

Due to hierarchical nature of the train data, features are generated by aggregating the data points for each of `bookingID`.
The generated features mainly contain basic statistical aggregation combined with some non-trivial feature engineering

Here are the list of the aggregated features:

|           Feature          |                                                            Description                                                           |
|:--------------------------:|:--------------------------------------------------------------------------------------------------------------------------------:|
| basic statistical feautres | mean, min, max, std, 25th percentile, median and 75th percentile of  all train dataset columns  except: `bookingID` and `second` |
| trip_duration              | total trip duration                                                                                                              |
| n_stops                    | calculates number of stops during a trip                                                                                         |
| hit_mean, hit_max, hit_std | the mean / max / std of speed differences when the vehicle stops                                                                 |
| naive_distance             | calculates the naive distance of at trip by simply summing all the `speed` column                                                |
| stopping_time_ratio        | gets the stopping ratio during the last 5% of the trip                                                                           |
| num_acceleration_change_x  | number of acceleration changes in x-axis                                                                                         |
| num_acceleration_change_y  | number of acceleration changes in y-axis                                                                                         |
| num_acceleration_change_z  | number of acceleration changes in z-axis                                                                                         |

### Modelling
For the code, head to [models](models/) folder for full information on the implementation

For modelling, ensembled model stacking approach is utilized. A High level diagram explaining the approach:

![alt text](docs/modelling.png "Model Stacking Diagram") <!-- .element height="100px" width="150px" -->
For the first layer, models such as: RandomForest, ExtraTrees, LightGBM and XGBoost are used to generate meta-features for the second layer training.
Each model is trained on a 5-Fold Stratified Cross Validation.
Finally, Ridge Regression is used to train the meta-features and generate the final predictions.

credits: part of the diagram was taken from <a href="https://www.kaggle.com/getting-started/18153#post103381">here</a>

### Model Training
If you want to see how the analysis + the model training are done. Please head to [analysis.ipynb](./analysis.ipynb).
Please note that if you want to retrain the model, you just need to make sure the input path to the `features` and `labels` folders
are correct. After the training was done, re-run `predict_test.py` to get your prediction CSV.
